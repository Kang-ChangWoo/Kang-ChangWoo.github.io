<!DOCTYPE html>
<html lang="en">
<head>
  <title>Changwoo Kang</title>
  <meta name="description" content="Personal webpage of Changwoo Kang" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes">
  <meta charset="utf-8">

  <!-- Open Graph -->
  <meta property="og:type" content="website"/>
  <meta property="og:url" content="https://kang-changwoo.github.io/"/>
  <meta property="og:title" content="Changwoo Kang"/>
  <meta property="og:description" content="3D Vision, Event-based Perception, and Multimodal AI"/>
  <meta property="og:image" content="https://kang-changwoo.github.io/asset/profile.JPG">

  <!-- CSS & Fonts -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
  <script src="https://kit.fontawesome.com/bacac70704.js" crossorigin="anonymous"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="shortcut icon" href="resrc/icons/profile_icon.jpg">
  <link href="style.css" rel="stylesheet">
</head>

<body>
  <!-- Header -->
  <div class="container" style="padding-top:2rem;">
    <div class="col-lg-6" style="text-align:left;">
      <h1 style="font-size:60px;">Changwoo Kang</h1> <!-- 65px -->
      <h3>Ph.D. Student @ <a href="https://unist.info/">3D Vision & Robotics Lab</a></h3>
      <!-- <a href="https://ai.unist.ac.kr/" style="font-weight:400;">Artificial Intelligence Graduate School</a><br> -->
     I am a Ph.D. student in the Artificial Intelligence Graduate School at 
    <a href="https://www.unist.ac.kr/" target="_blank">UNIST</a>, where I am a member of the 
    <a href="https://unist.info/" target="_blank">3D Vision &amp; Robotics Lab</a> advised by 
    <a href="https://unist.info/?page_id=194" target="_blank">Prof. Kyungdon Joo</a>. 
    My research interests lie in 3D vision, generative models, and multimodal learning across diverse sensory modalities.
      <!--<a href="https://www.unist.ac.kr/">Ulsan National Institute of Science and Technology (UNIST)</a><br><br> -->
      <div class="blank"></div>
      <p style="font-size: large;">
        üìç Ulsan, South Korea <br>
        <!--‚úâÔ∏è branden.c.w.kang@gmail.com <br> -->
        <!--‚úâÔ∏è kangchangwoo@unist.ac.kr <br> -->
        <!-- üåê <a href="https://kang-changwoo.github.io">https://kang-changwoo.github.io</a> -->
      </p>
    </div>

    <div class="col-lg-6" style="text-align:center; padding-top:2rem;">
      <!--<img src="./asset/profile.JPG" alt="Profile picture" style="height:300px;"> -->
      <img src="./asset/profile.JPG" alt="Profile picture" style="height:300px;" class="profile-img"><br>
      <h5 style="padding-top:5px;">
        <a href="mailto:branden.c.w.kang@gmail.com" title="Email"><i class="fa fa-envelope-square fa-3x"></i></a>
        <a href="https://github.com/Kang-ChangWoo" title="GitHub" target="_blank"><i class="fa fa-github-square fa-3x"></i></a>
        <a href="https://www.linkedin.com/in/changwoo-kang-13293214a" title="LinkedIn" target="_blank"><i class="fa fa-linkedin-square fa-3x"></i></a>
        <a href="https://scholar.google.com/citations?user=XXXXXX" title="Google Scholar" target="_blank"><i class="ai ai-google-scholar-square ai-3x"></i></a>
        <!-- <a href="./asset/cv.pdf" title="CV" target="_blank"><i class="fa fa-file-pdf-o fa-3x"></i></a> -->
        <a href="./asset/cv.pdf" title="CV" target="_blank"><i class="ai ai-cv-square ai-3x"></i></a>
      </h5>
    </div>
  </div>

  <!-- About -->
  <!--<div class="container">
    <h2>Introduction</h2>
    <hr/>
    <p>
      I am a Ph.D. student in the <b>3D Vision & Robotics Lab</b> at UNIST, advised by
      <a href="https://sites.google.com/view/kyungdonjoo">Prof. Kyung-Don Joo</a>.
      My research focuses on <b>3D shape generation</b> and <b>multimodal sensor fusion</b> using event cameras.
      I aim to design models that combine RGB-trained priors with event-based cues for
      <b>low-latency, robust 3D perception</b> under dynamic and low-light environments.
    </p>
  </div>-->

  <div class="blank"></div>
  
  <!-- Publications -->
  <div class="container">
    <h2>Publications</h2>
    <hr/>

    <!-- DogRecon -->
    <!--<div class="container" style="padding-top:1.5rem; padding-bottom:1.5rem;">
      <div class="col-lg-3" style="text-align:left; padding-top:1rem;">
        <img src="./asset/dogrecon.png" alt="DogRecon" style="width:100%; max-width:300px;">
      </div>
      <div class="col-lg-9" style="text-align:left;">
        <h4>DogRecon: Canine Prior-Guided Animatable 3D Gaussian Dog Reconstruction From a Single Image</h4>
        <p style="font-size:16px;"><i>International Journal of Computer Vision (IJCV), 2025</i></p>
        <p>
          Gyeongsu Cho, <b>Changwoo Kang</b>, Donghyeon Soon, Kyungdon Joo<br>
        </p>
      </div>
    </div> -->

<!-- ======== DogRecon ======== -->
<div class="container" style="padding-top:1.5rem; padding-bottom:1.5rem;">
  <div class="col-lg-3" style="text-align:left; padding-top:1rem;">
    <img src="./asset/dogrecon.png" alt="DogRecon" style="width:100%; max-width:300px;">
  </div>

  <div class="col-lg-9 pub-text" style="text-align:left;">
    <h4>DogRecon: Canine Prior-Guided Animatable 3D Gaussian Dog Reconstruction From a Single Image</h4>
    <p class="venue">International Journal of Computer Vision (IJCV), 2025</p>
    <p>Gyeongsu Cho, <b>Changwoo Kang</b>, Donghyeon Soon, Kyungdon Joo</p>
    <p>Our insight is to combine the acquisition of appearance from generative models, without additional data, with geometric guidance provided by a parametric representation, aiming to achieve complete geometry. Thus, we present DogRecon, our framework consists of two key components: Canine-centric novel view synthesis with canine prior for multi-view generation of dog and a reliable sampling weight strategy with Gaussian Splatting for animatable 3D dog reconstruction.
    </p>
    <p>
      <a href="https://vision3d-lab.github.io/dogrecon/" target="_blank"> üìÇ Project Page</a> | <!-- <i class="fa fa-link"></i> -->
      <a href="https://drive.google.com/file/d/1Nh0t78usBeoX7UcOKU18FZ3-pQ0ZECjq/view" target="_blank"> üé• Video</a> | <!-- <i class="fa fa-code"></i> -->
      <a href="https://link.springer.com/article/10.1007/s11263-025-02485-5" target="_blank"> üìÑ Paper</a> <!-- <i class="fa fa-file-pdf-o"></i> -->
    </p>
  </div>
</div>

<!-- ======== ContactGen ======== -->
<div class="container" style="padding-top:1.5rem; padding-bottom:1.5rem;">
  <div class="col-lg-3" style="text-align:left; padding-top:1rem;">
    <img src="./asset/contactgen.png" alt="ContactGen" style="width:100%; max-width:300px;">
  </div>

  <div class="col-lg-9 pub-text" style="text-align:left;">
    <h4>ContactGen: Contact-Guided Interactive 3D Human Generation for Partners</h4>
    <p class="venue">AAAI Conference on Artificial Intelligence (AAAI), 2024</p>
    <p>Dongjun Gu, Jaehyeok Shim, Jaehoon Jang, <b>Changwoo Kang</b>, Kyungdon Joo</p>
    <p>
      We propose a novel method of generating interactive 3D humans for a given partner human based on a guided diffusion framework (ContactGen in short). Specifically, we newly present a contact prediction module that adaptively estimates potential contact regions between two input humans according to the interaction label. Using the estimated potential contact regions as complementary guidances, we dynamically enforce ContactGen to generate interactive 3D humans for a given partner human within a guided diffusion model. 
    </p>
    <p>
      <a href="https://dongjunku.github.io/contactgen/" target="_blank"> üìÇ Project Page</a> | <!-- <i class="fa fa-link"></i> -->
      <a href="https://github.com/dongjunKu/ContactGen/" target="_blank"> üß† Code</a> | <!--  <i class="fa fa-code"></i> -->
      <a href="https://ojs.aaai.org/index.php/AAAI/article/view/27962" target="_blank"> üìÑ Paper</a> <!-- <i class="fa fa-file-pdf-o"></i> -->
    </p>
  </div>
</div>

<!-- ======== Diffusion SDF ======== -->
<div class="container" style="padding-top:1.5rem; padding-bottom:1.5rem;">
  <div class="col-lg-3" style="text-align:left; padding-top:1rem;">
    <img src="./asset/sdfgen.png" alt="Diffusion SDF" style="width:100%; max-width:300px;">
  </div>

  <div class="col-lg-9 pub-text" style="text-align:left;">
    <h4>Diffusion-Based Signed Distance Fields for 3D Shape Generation</h4>
    <p class="venue">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023</p>
    <p>Jaehyeok Shim, <b>Changwoo Kang</b>, Kyungdon Joo</p>
    <p>Our framework generates high-resolution 3D shapes while alleviating memory issues by separating the generative process into two-stage: generation and super-resolution.  In the first stage, a diffusion-based generative model generates a low-resolution SDF of 3D shapes. Using the estimated low-resolution SDF as a condition, patch-based diffusion model performs super-resolution in the second stage. the later diffusion model disturbs high-resolution noise to synthesis complete shape.
    </p>
    <p>
      <a href="https://kitsunetic.github.io/sdf-diffusion/" target="_blank"> üìÇ Project Page</a> | <!-- <i class="fa fa-link"></i> -->
      <a href="https://github.com/Kitsunetic/SDF-Diffusion" target="_blank"> üß† Code</a> | <!-- <i class="fa fa-code"></i> -->
      <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Shim_Diffusion-Based_Signed_Distance_Fields_for_3D_Shape_Generation_CVPR_2023_paper.html" target="_blank"> üìÑ Paper</a> <!-- <i class="fa fa-file-pdf-o"></i> -->
    </p>
  </div>
</div>

<!-- ======== Pose-Guided 3D Human ======== -->
<div class="container" style="padding-top:1.5rem; padding-bottom:1.5rem;">
  <div class="col-lg-3" style="text-align:left; padding-top:1rem;">
    <img src="./asset/humangen.png" alt="Pose-Guided 3D Human Generation" style="width:100%; max-width:300px;">
  </div>

  <div class="col-lg-9 pub-text" style="text-align:left;">
    <h4>Pose-Guided 3D Human Generation in Indoor Scene</h4>
    <p class="venue">AAAI Conference on Artificial Intelligence (AAAI), 2023</p>
    <p>Minseok Kim, <b>Changwoo Kang</b>, Jeongin Park, Kyungdon Joo</p>
    <p>
      We present a new 3D human generation framework that considers geometric alignment on potential contact areas between 3D human avatars and their surroundings. In addition, we introduce a compact yet effective human pose classifier that classifies the human pose and provides potential contact areas of the 3D human avatar. It allows us to adaptively use geometric alignment loss according to the classified human pose.
    </p>
    <p>
      <a href="https://bupyeonghealer.github.io/phin/" target="_blank"> üìÇ Project Page</a> | <!-- <i class="fa fa-link"></i> -->
      <a href="https://www.youtube.com/watch?v=FL8oc74jDlc&feature=youtu.be" target="_blank"> üé• Video</a> | <!-- <i class="fa fa-code"></i> -->üíæ
      <a href="https://drive.google.com/drive/folders/1qbl6YMU3Xa9tJsN7JDOt0myezEvQ0n27" target="_blank"> üíæ Dataset</a> | <!-- <i class="fa fa-code"></i> -->
      <a href="https://ojs.aaai.org/index.php/AAAI/article/view/25195" target="_blank"> üìÑ Paper</a> <!-- <i class="fa fa-file-pdf-o"></i> -->
    </p>
  </div>
</div>
 
  <div class="blank"></div>
  
  <!-- Projects -->
  <div class="container">
    <h2>Projects</h2>
    <hr/>
    <ul>
      <li>Event Camera-Centric Fusion Sensor Pack for Transferable 3D Perception among Heterogeneous Agents, funded by NRF (2024‚Äìpresent)</li>
      <li>Geometric and Physical Commonsense Reasoning based Behavior Intelligence for Embodied AI, funded by IITP (2022‚Äìpresent)</li>
      <li>Vision System for Vibration Analysis, funded by RIST (2022‚Äì2023)</li>
      <li>Virtual Tactile Feedback Recommendation, funded by UNIST (2022‚Äì2023)</li>
    </ul>
  </div>

<div class="blank"></div>
  
  <!-- Experience -->
  <div class="container">
    <h2>Experience</h2>
    <hr/>
    <ul>
      <li>
        <b>2025.06 ‚Äì 2025.08</b>: 2-month research stay in Singapore, working on event-driven 3D perception
        and multimodal fusion for robust sensing under challenging conditions.
      </li>
    </ul>
  </div>

  <div class="blank"></div>
  <div class="blank"></div>
  <div class="blank"></div>
  
  <!-- Footer + Visitor Map -->
  <div class="footer-wrapper">
    <p>&copy; Template adapted from <a href="https://anttwo.github.io/"> Antoine Gu√©don.</a></p>

    <!-- MapMyVisitors 2D World Map Widget -->
    <script type="text/javascript" id="mapmyvisitors" src="https://mapmyvisitors.com/map.js?cl=20284d&w=a&t=n&d=3tiBLIyk5FYO3FPjwYYcq7iGYneStJzPj8soeWJivwc&co=ffffff&ct=ffffff&cmo=ffd753&cmn=cc3a5f"></script>
  </div>
  
</body>
</html>


















































